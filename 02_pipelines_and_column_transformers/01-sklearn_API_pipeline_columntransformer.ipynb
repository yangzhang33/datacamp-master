{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced scikit-learn\n",
    "\n",
    "```\n",
    "Authors: Alexandre Gramfort\n",
    "         Thomas Moreau\n",
    "```\n",
    "\n",
    "The aim of this notebook is:\n",
    "\n",
    "  - to explain the **full scikit-learn API** (estimators, transformers, classifiers, regressors, splitters)\n",
    "  - to explain how to assemble these objects in complex **pipelines with mixed data types** (numerical, categorical etc.) using **Pipeline** and **ColumnTransformer objects**.\n",
    "  - have you **write your own transformer, splitter and classifier**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explain these concepts we will start from a full working code based on the Titanic dataset. Then we will deconstruct all the blocks and start writing our own Python classes.\n",
    "\n",
    "\n",
    "First, let's fetch the \"titanic\" dataset directly from OpenML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>ticket</th>\n",
       "      <th>fare</th>\n",
       "      <th>cabin</th>\n",
       "      <th>embarked</th>\n",
       "      <th>boat</th>\n",
       "      <th>body</th>\n",
       "      <th>home.dest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Allen, Miss. Elisabeth Walton</td>\n",
       "      <td>female</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24160</td>\n",
       "      <td>211.3375</td>\n",
       "      <td>B5</td>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>St Louis, MO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Allison, Master. Hudson Trevor</td>\n",
       "      <td>male</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Montreal, PQ / Chesterville, ON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Allison, Miss. Helen Loraine</td>\n",
       "      <td>female</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Montreal, PQ / Chesterville, ON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Allison, Mr. Hudson Joshua Creighton</td>\n",
       "      <td>male</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>None</td>\n",
       "      <td>135.0</td>\n",
       "      <td>Montreal, PQ / Chesterville, ON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Allison, Mrs. Hudson J C (Bessie Waldo Daniels)</td>\n",
       "      <td>female</td>\n",
       "      <td>25.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Montreal, PQ / Chesterville, ON</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pclass                                             name     sex      age  \\\n",
       "0     1.0                    Allen, Miss. Elisabeth Walton  female  29.0000   \n",
       "1     1.0                   Allison, Master. Hudson Trevor    male   0.9167   \n",
       "2     1.0                     Allison, Miss. Helen Loraine  female   2.0000   \n",
       "3     1.0             Allison, Mr. Hudson Joshua Creighton    male  30.0000   \n",
       "4     1.0  Allison, Mrs. Hudson J C (Bessie Waldo Daniels)  female  25.0000   \n",
       "\n",
       "   sibsp  parch  ticket      fare    cabin embarked  boat   body  \\\n",
       "0    0.0    0.0   24160  211.3375       B5        S     2    NaN   \n",
       "1    1.0    2.0  113781  151.5500  C22 C26        S    11    NaN   \n",
       "2    1.0    2.0  113781  151.5500  C22 C26        S  None    NaN   \n",
       "3    1.0    2.0  113781  151.5500  C22 C26        S  None  135.0   \n",
       "4    1.0    2.0  113781  151.5500  C22 C26        S  None    NaN   \n",
       "\n",
       "                         home.dest  \n",
       "0                     St Louis, MO  \n",
       "1  Montreal, PQ / Chesterville, ON  \n",
       "2  Montreal, PQ / Chesterville, ON  \n",
       "3  Montreal, PQ / Chesterville, ON  \n",
       "4  Montreal, PQ / Chesterville, ON  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df, y = fetch_openml(\"titanic\", version=1, as_frame=True, return_X_y=True)\n",
    "X_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification task is to predict whether or not a person will survive the Titanic disaster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       1\n",
       "2       0\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "1304    0\n",
       "1305    0\n",
       "1306    0\n",
       "1307    0\n",
       "1308    0\n",
       "Name: survived, Length: 1309, dtype: category\n",
       "Categories (2, object): ['0', '1']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split the data into a training and a testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_df, y, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(981, 13) (328, 13)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <p><b>QUESTIONS</b>:</p>\n",
    "    <ul>\n",
    "        <li>What would happen if you try to fit a <tt>RandomForestClassifier</tt>?</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Rekic, Mr. Tido'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mf:\\IP Paris\\datacamp-master\\02_pipelines_and_column_transformers\\01-sklearn_API_pipeline_columntransformer.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/IP%20Paris/datacamp-master/02_pipelines_and_column_transformers/01-sklearn_API_pipeline_columntransformer.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mensemble\u001b[39;00m \u001b[39mimport\u001b[39;00m RandomForestClassifier\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/IP%20Paris/datacamp-master/02_pipelines_and_column_transformers/01-sklearn_API_pipeline_columntransformer.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m RandomForestClassifier(n_estimators\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/IP%20Paris/datacamp-master/02_pipelines_and_column_transformers/01-sklearn_API_pipeline_columntransformer.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train,y_train)\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\ima\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:327\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m issparse(y):\n\u001b[0;32m    326\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 327\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    328\u001b[0m     X, y, multi_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mDTYPE\n\u001b[0;32m    329\u001b[0m )\n\u001b[0;32m    330\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    331\u001b[0m     sample_weight \u001b[39m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\ima\\lib\\site-packages\\sklearn\\base.py:581\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    579\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[0;32m    580\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 581\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    582\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    584\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\ima\\lib\\site-packages\\sklearn\\utils\\validation.py:964\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    961\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    962\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39my cannot be None\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 964\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m    965\u001b[0m     X,\n\u001b[0;32m    966\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[0;32m    967\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[0;32m    968\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    969\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[0;32m    970\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m    971\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[0;32m    972\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[0;32m    973\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[0;32m    974\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[0;32m    975\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[0;32m    976\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m    977\u001b[0m )\n\u001b[0;32m    979\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric)\n\u001b[0;32m    981\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\ima\\lib\\site-packages\\sklearn\\utils\\validation.py:746\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    744\u001b[0m         array \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mastype(dtype, casting\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39munsafe\u001b[39m\u001b[39m\"\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    745\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 746\u001b[0m         array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m    747\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[0;32m    748\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    749\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    750\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\ima\\lib\\site-packages\\pandas\\core\\generic.py:2064\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   2063\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype: npt\u001b[39m.\u001b[39mDTypeLike \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m-> 2064\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49masarray(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_values, dtype\u001b[39m=\u001b[39;49mdtype)\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'Rekic, Mr. Tido'"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "model.fit(X_train,y_train)\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working only with numerical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a model using only numerical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pclass        float64\n",
       "name           object\n",
       "sex          category\n",
       "age           float64\n",
       "sibsp         float64\n",
       "parch         float64\n",
       "ticket         object\n",
       "fare          float64\n",
       "cabin          object\n",
       "embarked     category\n",
       "boat           object\n",
       "body          float64\n",
       "home.dest      object\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using scikit-learn, we will do some simple preprocessing using pandas. First, let's select only a few the numerical columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['pclass', 'age', 'parch', 'fare']\n",
    "\n",
    "X_train_num = X_train[num_cols]\n",
    "X_test_num = X_test[num_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <p><b>QUESTIONS</b>:</p>\n",
    "    <ul>\n",
    "        <li>What would happen if you try to fit a <tt>RandomForestClassifier</tt>?</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mf:\\IP Paris\\datacamp-master\\02_pipelines_and_column_transformers\\01-sklearn_API_pipeline_columntransformer.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/IP%20Paris/datacamp-master/02_pipelines_and_column_transformers/01-sklearn_API_pipeline_columntransformer.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m RandomForestClassifier(n_estimators\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/IP%20Paris/datacamp-master/02_pipelines_and_column_transformers/01-sklearn_API_pipeline_columntransformer.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train_num, y_train)\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\ima\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:327\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m issparse(y):\n\u001b[0;32m    326\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 327\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    328\u001b[0m     X, y, multi_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mDTYPE\n\u001b[0;32m    329\u001b[0m )\n\u001b[0;32m    330\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    331\u001b[0m     sample_weight \u001b[39m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\ima\\lib\\site-packages\\sklearn\\base.py:581\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    579\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[0;32m    580\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 581\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    582\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    584\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\ima\\lib\\site-packages\\sklearn\\utils\\validation.py:964\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    961\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    962\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39my cannot be None\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 964\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m    965\u001b[0m     X,\n\u001b[0;32m    966\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[0;32m    967\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[0;32m    968\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    969\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[0;32m    970\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m    971\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[0;32m    972\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[0;32m    973\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[0;32m    974\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[0;32m    975\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[0;32m    976\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m    977\u001b[0m )\n\u001b[0;32m    979\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric)\n\u001b[0;32m    981\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\ima\\lib\\site-packages\\sklearn\\utils\\validation.py:800\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    794\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    795\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    796\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    797\u001b[0m         )\n\u001b[0;32m    799\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 800\u001b[0m         _assert_all_finite(array, allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    802\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    803\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\ima\\lib\\site-packages\\sklearn\\utils\\validation.py:114\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    108\u001b[0m         allow_nan\n\u001b[0;32m    109\u001b[0m         \u001b[39mand\u001b[39;00m np\u001b[39m.\u001b[39misinf(X)\u001b[39m.\u001b[39many()\n\u001b[0;32m    110\u001b[0m         \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_nan\n\u001b[0;32m    111\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misfinite(X)\u001b[39m.\u001b[39mall()\n\u001b[0;32m    112\u001b[0m     ):\n\u001b[0;32m    113\u001b[0m         type_err \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minfinity\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m allow_nan \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mNaN, infinity\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 114\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    115\u001b[0m             msg_err\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    116\u001b[0m                 type_err, msg_dtype \u001b[39mif\u001b[39;00m msg_dtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m X\u001b[39m.\u001b[39mdtype\n\u001b[0;32m    117\u001b[0m             )\n\u001b[0;32m    118\u001b[0m         )\n\u001b[0;32m    119\u001b[0m \u001b[39m# for object dtype data, we only check for NaNs (GH-13254)\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[39melif\u001b[39;00m X\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mdtype(\u001b[39m\"\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_nan:\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=100)\n",
    "model.fit(X_train_num, y_train)\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might want to look into a summary of the data that we try to fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are some missing data, we can replace them with a mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num_imputed = X_train_num.fillna(X_train_num.mean())\n",
    "X_train_num_imputed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_num_imputed, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <p><b>EXERCISE</b>:</p>\n",
    "    <ul>\n",
    "    <li>What should we do if there are also missing values in the test set?</li>\n",
    "    <li>Process the test set so as to be able to compute the test score of the model.</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "Solution is in `solutions/01-pandas_fillna_test.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make it less error prone using scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn provides some \"transformers\" to preprocess the data. `sklearn.impute.SimpleImputer` is a transformer allowing for the same job than the processing done with Pandas. However, we will see later that it integrates greatly with other scikit-learn components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As any estimator in scikit-learn, a transformer has a `fit` method which should be called on the training data to learn the required statistics. In the case of a mean imputer, we need to compute the mean for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.fit(X_train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To impute the values by the mean, we can use the `transform` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.transform(X_train_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previoulsy mentioned, we should impute with the values computed in `fit` when imputing the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>What is a \"Transformer\"?</b>: <br/>\n",
    "\n",
    "A scikit-learn transform should implement at least these methods:\n",
    "\n",
    "<ul>\n",
    "    <li>fit(X, y=None)</li>\n",
    "    <li>transform(X)</li>\n",
    "    <li>get_params()</li>\n",
    "    <li>set_params(**kwargs)</li>  \n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = imputer.get_params()\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.transform?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the attributes of our `imputer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_attributes = [attr for attr in dir(imputer) if not attr.startswith('_')]\n",
    "public_attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have among these attributes:\n",
    "\n",
    "- **parameters** (keys in get_params method output)\n",
    "- **methods** (fit, transform, etc.)\n",
    "- **estimated quantities** that appear after a `fit` (ending with `_`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_methods = [\n",
    "    attr for attr in dir(imputer)\n",
    "    if not attr.startswith('_') and\n",
    "    not attr.endswith('_') and\n",
    "    attr not in params]\n",
    "public_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.inverse_transform?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated quantities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_attributes = [\n",
    "    attr for attr in dir(imputer)\n",
    "    if not attr.startswith('_') and\n",
    "    attr.endswith('_')]\n",
    "fit_attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <p><b>EXERCISE</b>:</p>\n",
    "    <ul>\n",
    "        <li>What are the attributes of a RandomForestClassifier. You will decompose these in the 3 categories.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Pipeline\n",
    "\n",
    "We saw earlier that we should be careful when preprocessing data to avoid any \"data leak\" (i.e. reusing some knowledge from the training when testing our model). Scikit-learn provides the `Pipeline` class to make successive transformations. In addition, it will ensure that the right operations will be applied at the right time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "\n",
    "set_config(display='diagram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model = make_pipeline(SimpleImputer(strategy='mean'),\n",
    "                      RandomForestClassifier(n_estimators=200))\n",
    "model.fit(X_train_num, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative syntax using named \"steps\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "model = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy='mean')),\n",
    "    (\"classifier\", RandomForestClassifier(n_estimators=200))    \n",
    "])\n",
    "model.fit(X_train_num, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test_num, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving your estimator in HTML for presentations, blog posts etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import estimator_html_repr\n",
    "\n",
    "with open('model.html', 'w') as fid:\n",
    "    fid.write(estimator_html_repr(model))\n",
    "\n",
    "# !open model.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulating Pipeline steps\n",
    "\n",
    "A pipeline is a sequence of `steps`. Each `step` is a scikit-learn estimator. All steps except the last one are typically **transformers** (fit, fit_transform, transform methods) and the last step is a **classifier** or a **regressor**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.steps  # accessing steps as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.named_steps  # accessing steps with their names as a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model[:1]  # slicing a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import is_classifier\n",
    "\n",
    "is_classifier(model[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's decompose the pipeline and chain the operations manually (mimicking what the Pipeline object does internally):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = model[:-1]\n",
    "classifier = model[-1]\n",
    "\n",
    "X_train_preproc = preprocessor.fit_transform(X_train_num, y_train)\n",
    "X_test_preproc = preprocessor.transform(X_test_num)\n",
    "\n",
    "classifier.fit(X_train_preproc, y_train)\n",
    "classifier.score(X_test_preproc, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Towards a ColumnTransformer\n",
    "\n",
    "If we want to directly fit the model on `X_train`, we can select the numerical columns using  a `ColumnTransformer` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "ColumnTransformer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_preprocessing = ColumnTransformer([\n",
    "    (\"imputer\", SimpleImputer(strategy='mean'), num_cols)\n",
    "])\n",
    "    \n",
    "model = Pipeline([\n",
    "    (\"numerical preproc.\", numerical_preprocessing),\n",
    "    (\"classifier\", RandomForestClassifier(n_estimators=100)),\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working only with categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical columns (even more string data types) are not supported natively by machine-learning algorithms and required some preprocessing step usually called encoding. The most classical categorical encoders are the `OrdinalEncoder` and the `OneHotEncoder`. Let's first see the `OrdinalEncoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['sex', 'embarked', 'pclass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cat = X_train[cat_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    (\"ordinal_encoder\", OrdinalEncoder())\n",
    "])\n",
    "\n",
    "categorical_preprocessing = ColumnTransformer([\n",
    "    (\"categorical_preproc\", cat_pipeline, cat_cols)\n",
    "])\n",
    "\n",
    "model = Pipeline([\n",
    "    (\"categorical_preproc\", categorical_preprocessing),\n",
    "    (\"classifier\", RandomForestClassifier(n_estimators=100))\n",
    "])\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing and updating parameters of a Pipeline\n",
    "\n",
    "Pipeline is yet another scikit-learn estimators. It therefore has the methods `set_params` and `get_params`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_params(classifier__n_estimators=666)\n",
    "model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <p><b>EXERCISE</b>:</p>\n",
    "    There are many other types of ways to encode categorical variables.\n",
    "    <ul>\n",
    "        <li>Write your own categorical encoder CountEncoder. The idea is to replace categorical variables with their count in the train set.</li>\n",
    "        <li>Change the ordinal encoder in the pipeline above with an instance of your CountEncoder.</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "Your class will need to inherit from `BaseEstimator`, `TransformerMixin` in `sklearn.base` submodule.\n",
    "You will use the class `Counter` from the collections module in the standard library.\n",
    "\n",
    "You will your code on this toy example\n",
    "\n",
    "```python\n",
    ">>> X = np.array([\n",
    "...    [0, 2],\n",
    "...    [1, 3],\n",
    "...    [1, 1],\n",
    "...    [1, 1],\n",
    "... ])\n",
    ">>> ce = CountEncoder()\n",
    ">>> print(ce.fit_transform(X))\n",
    "[[1 1]\n",
    " [3 1]\n",
    " [3 2]\n",
    " [3 2]]\n",
    "```\n",
    "\n",
    "Solution is in `solutions/01-count_encoder.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining both categorical and numerical data in the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <p><b>EXERCISE</b>:</p>\n",
    "    <ul>\n",
    "    <li>Try to combine the numerical and categorical pipelines into a single <tt>ColumnTransformer</tt></li>\n",
    "        <li>Fit a <tt>RandomForestClassifier</tt> on the output of this feature engineering. How does the test score evolve?</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "Solution is in `solutions/01b-full_column_transformer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    (\"count_encoder\", OrdinalEncoder())\n",
    "])\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "])\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    (\"categorical_preproc\", cat_pipeline, cat_cols),\n",
    "    (\"numerical_preproc\", num_pipeline, num_cols)\n",
    "])\n",
    "\n",
    "model = Pipeline([\n",
    "    (\"preproc\", preprocessing),\n",
    "    (\"classifier\", RandomForestClassifier(n_estimators=100))\n",
    "])\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From one split to cross-validation (CV): splitters!\n",
    "\n",
    "CV objects are parametrized to split data in multiple train/test splits.\n",
    "\n",
    "A splitter should implement a `split` method.\n",
    "\n",
    "Given a `model`, some data `X, y` and a splitter one can fit and score on\n",
    "all requested data splits. Functions to do this are `cross_val_score`\n",
    "(historical way) and `cross_validate` (more modern way).\n",
    "\n",
    "Let's first define a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"ordinal_encoder\", OrdinalEncoder())\n",
    "])\n",
    "\n",
    "categorical_preprocessing = ColumnTransformer([\n",
    "    (\"categorical_preproc\", cat_pipeline, cat_cols)\n",
    "])\n",
    "\n",
    "model = Pipeline([\n",
    "    (\"categorical_preproc\", categorical_preprocessing),\n",
    "    (\"classifier\", RandomForestClassifier(n_estimators=100))\n",
    "])\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now use a default 5-fold CV. You will see that there is a large amount of discrepancy among the test_score values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "cv_results = cross_validate(model, X_df, y, cv=5)\n",
    "pd.DataFrame(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cv_results).agg(['mean', 'std'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason is that default CV object (here 5-fold CV) is deterministic, while the distribution of \"survivor\" is not uniform in the dataset. See:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.astype(int).to_frame().groupby(y.index.values // 100).mean().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fix this one needs stratified folds (so that the fraction of \"survivors\" is the same in each fold) but also to shuffle the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_results = cross_validate(model, X_df, y, cv=cv)\n",
    "pd.DataFrame(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cv_results).agg(['mean', 'std'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variance across folds is now much smaller which is great!\n",
    "\n",
    "Let's look at the cross-validation scheme with a pretty plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting_utils import plot_cv_indices\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(9, 4))\n",
    "\n",
    "for shuffle, ax in zip([False, True], axes):\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=shuffle)\n",
    "    plot_cv_indices(cv, X_df, y, ax=ax)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html for more details and a list of the different CV objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing your own cross-validation object\n",
    "\n",
    "A splitter should implement the `split` and `get_n_splits` method. The `split` should return an iterable of tuples of indices and `get_n_splits` an integer corresponding to the number of splits/folds. If you know about `yield` and Python generators you can use these.\n",
    "\n",
    "Let's first see what we get with the `cv` object above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.get_n_splits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_idx, test_idx in cv.split(X_df, y):\n",
    "    print(train_idx[:5], test_idx[:5])\n",
    "    print(f\"N. samples train: {len(train_idx)}  -- N. samples test: {len(test_idx)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <p><b>EXERCISE</b>:</p>\n",
    "    <ul>\n",
    "        <li>Imagine that the index of <code>y</code> gives you some provenance about the sample (e.g. which cohort of subjects in a clinical study). Write a splitter that allows to test the performance of a model on a left-out cohort. In other words, you will do as many splits as the number of unique values in <code>y.index.values</code>, and predict of each left-out cohort. To simulate this, we will modify the index variable <code>y</code>, just for educational purposes.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "Solution is in `solutions/01c-splitter.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_with_provenance = y.copy()\n",
    "y_with_provenance.index = y_with_provenance.index.values // 200  # to easily mimic 5 cohorts\n",
    "n_splits = y_with_provenance.index.nunique()\n",
    "print(n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ProvenanceSplitter():\n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return y.index.nunique()\n",
    "    \n",
    "    def split(self, X, y):\n",
    "        indices = np.arange(X.shape[0])\n",
    "        n_splits = self.get_n_splits(y=y)\n",
    "        res = []\n",
    "        for i in y.index.unique():\n",
    "            idx_train = indices\n",
    "            idx_test = indices[y.index == i]\n",
    "            yield idx_train, idx_test\n",
    "            # res.append((idx_train, idx_test))\n",
    "        # return res\n",
    "    \n",
    "cv = ProvenanceSplitter()\n",
    "for train_idx, test_idx in cv.split(X_df, y_with_provenance):\n",
    "    print(y_with_provenance.iloc[train_idx[:5]].unique(), y_with_provenance.iloc[test_idx[:5]].unique())\n",
    "    print(f\"N. samples train: {len(train_idx)}  -- N. samples test: {len(test_idx)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When you're done you can look at the assignments on scikit-learn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ima",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "ac56bd6234faaedadba2d2dcf914858e2148c6cfbf00041937062ee96b89120b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
